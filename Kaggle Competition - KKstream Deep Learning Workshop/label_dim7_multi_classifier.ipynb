{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_id(n, val_percent=0.1):\n",
    "    ids = [i for i in range(n)]\n",
    "    random.shuffle(ids)\n",
    "    piv = int(n*val_percent)\n",
    "    if piv==0: \n",
    "        piv=1\n",
    "    return {'train': ids[:-piv], 'val': ids[-piv:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = np.load('train_features.npy')\n",
    "labels = np.load('train_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DATA_USE = 5000\n",
    "INPUT_DIM_1D = datas.shape[1]*datas.shape[2]\n",
    "OUTPUT_DIM = labels.shape[1]-1\n",
    "val_percent = 0.2\n",
    "\n",
    "train_val_id = split_train_val_id(NUM_DATA_USE, val_percent)\n",
    "train_id = train_val_id['train']\n",
    "val_id = train_val_id['val']\n",
    "\n",
    "train_datas = np.zeros((len(train_id), INPUT_DIM_1D))\n",
    "train_labels = np.zeros((len(train_id), OUTPUT_DIM))\n",
    "for i, idx in enumerate(train_id):\n",
    "    train_datas[i] = datas[idx].reshape(INPUT_DIM_1D)\n",
    "    train_labels[i] = labels[idx][1:]\n",
    "    \n",
    "#reduce data information\n",
    "train_datas = train_datas[:, :10000]\n",
    "\n",
    "\n",
    "# #seven labels\n",
    "day1_labels = np.zeros(len(train_id))\n",
    "day2_labels = np.zeros(len(train_id))\n",
    "day3_labels = np.zeros(len(train_id))\n",
    "day4_labels = np.zeros(len(train_id))\n",
    "day5_labels = np.zeros(len(train_id))\n",
    "day6_labels = np.zeros(len(train_id))\n",
    "day7_labels = np.zeros(len(train_id))\n",
    "\n",
    "for i in range(len(train_id)):\n",
    "    if train_labels[i, :4].sum()!=0:\n",
    "        day1_labels[i] = 1\n",
    "    if train_labels[i, 4:8].sum()!=0:\n",
    "        day2_labels[i] = 1\n",
    "    if train_labels[i, 8:12].sum()!=0:\n",
    "        day3_labels[i] = 1\n",
    "    if train_labels[i, 12:16].sum()!=0:\n",
    "        day4_labels[i] = 1\n",
    "    if train_labels[i, 16:20].sum()!=0:\n",
    "        day5_labels[i] = 1\n",
    "    if train_labels[i, 20:24].sum()!=0:\n",
    "        day6_labels[i] = 1\n",
    "    if train_labels[i, 24:28].sum()!=0:\n",
    "        day7_labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_solver = 'adam'\n",
    "_activation = 'relu'\n",
    "_alpha = 1e-4\n",
    "_hidden_layer = (256)\n",
    "_lr_rate = 0.001\n",
    "_batch = 64\n",
    "_max_iter = 200\n",
    "\n",
    "mlp1 = MLPClassifier(solver=_solver, activation=_activation, alpha=_alpha, hidden_layer_sizes=_hidden_layer,\\\n",
    "                    batch_size=_batch, learning_rate_init=_lr_rate, max_iter=_max_iter)\n",
    "mlp2 = MLPClassifier(solver=_solver, activation=_activation, alpha=_alpha, hidden_layer_sizes=_hidden_layer,\\\n",
    "                    batch_size=_batch, learning_rate_init=_lr_rate, max_iter=_max_iter)\n",
    "mlp3 = MLPClassifier(solver=_solver, activation=_activation, alpha=_alpha, hidden_layer_sizes=_hidden_layer,\\\n",
    "                    batch_size=_batch, learning_rate_init=_lr_rate, max_iter=_max_iter)\n",
    "mlp4 = MLPClassifier(solver=_solver, activation=_activation, alpha=_alpha, hidden_layer_sizes=_hidden_layer,\\\n",
    "                    batch_size=_batch, learning_rate_init=_lr_rate, max_iter=_max_iter)\n",
    "mlp5 = MLPClassifier(solver=_solver, activation=_activation, alpha=_alpha, hidden_layer_sizes=_hidden_layer,\\\n",
    "                    batch_size=_batch, learning_rate_init=_lr_rate, max_iter=_max_iter)\n",
    "mlp6 = MLPClassifier(solver=_solver, activation=_activation, alpha=_alpha, hidden_layer_sizes=_hidden_layer,\\\n",
    "                    batch_size=_batch, learning_rate_init=_lr_rate, max_iter=_max_iter)\n",
    "mlp7 = MLPClassifier(solver=_solver, activation=_activation, alpha=_alpha, hidden_layer_sizes=_hidden_layer,\\\n",
    "                    batch_size=_batch, learning_rate_init=_lr_rate, max_iter=_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp1 finish\n",
      "mlp2 finish\n",
      "mlp3 finish\n",
      "mlp4 finish\n",
      "mlp5 finish\n",
      "mlp6 finish\n",
      "mlp7 finish\n"
     ]
    }
   ],
   "source": [
    "mlp1.fit(train_datas, day1_labels)\n",
    "print('mlp1 finish')\n",
    "mlp2.fit(train_datas, day2_labels)\n",
    "print('mlp2 finish')\n",
    "mlp3.fit(train_datas, day3_labels)\n",
    "print('mlp3 finish')\n",
    "mlp4.fit(train_datas, day4_labels)\n",
    "print('mlp4 finish')\n",
    "mlp5.fit(train_datas, day5_labels)\n",
    "print('mlp5 finish')\n",
    "mlp6.fit(train_datas, day6_labels)\n",
    "print('mlp6 finish')\n",
    "mlp7.fit(train_datas, day7_labels)\n",
    "print('mlp7 finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_datas, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_VAL = len(val_id)\n",
    "val_datas = np.zeros((NUM_OF_VAL, INPUT_DIM_1D))\n",
    "val_labels = np.zeros((NUM_OF_VAL, OUTPUT_DIM))\n",
    "for i, idx in enumerate(val_id):\n",
    "    val_datas[i] = datas[idx].reshape(INPUT_DIM_1D)\n",
    "    val_labels[i] = labels[idx][1:]\n",
    "    \n",
    "#reduce data information\n",
    "val_datas = val_datas[:, :10000]\n",
    "\n",
    "# #seven labels\n",
    "val_day1_labels = np.zeros(NUM_OF_VAL)\n",
    "val_day2_labels = np.zeros(NUM_OF_VAL)\n",
    "val_day3_labels = np.zeros(NUM_OF_VAL)\n",
    "val_day4_labels = np.zeros(NUM_OF_VAL)\n",
    "val_day5_labels = np.zeros(NUM_OF_VAL)\n",
    "val_day6_labels = np.zeros(NUM_OF_VAL)\n",
    "val_day7_labels = np.zeros(NUM_OF_VAL)\n",
    "\n",
    "for i in range(NUM_OF_VAL):\n",
    "    if val_labels[i, :4].sum()!=0:\n",
    "        val_day1_labels[i] = 1\n",
    "    if val_labels[i, 4:8].sum()!=0:\n",
    "        val_day2_labels[i] = 1\n",
    "    if val_labels[i, 8:12].sum()!=0:\n",
    "        val_day3_labels[i] = 1\n",
    "    if val_labels[i, 12:16].sum()!=0:\n",
    "        val_day4_labels[i] = 1\n",
    "    if val_labels[i, 16:20].sum()!=0:\n",
    "        val_day5_labels[i] = 1\n",
    "    if val_labels[i, 20:24].sum()!=0:\n",
    "        val_day6_labels[i] = 1\n",
    "    if val_labels[i, 24:28].sum()!=0:\n",
    "        val_day7_labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.90      0.89       776\n",
      "         1.0       0.64      0.61      0.62       224\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.76      0.76      0.76      1000\n",
      "weighted avg       0.83      0.83      0.83      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.90      0.91       787\n",
      "         1.0       0.65      0.70      0.67       213\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.78      0.80      0.79      1000\n",
      "weighted avg       0.86      0.85      0.86      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.91      0.91       792\n",
      "         1.0       0.64      0.62      0.63       208\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.77      0.76      0.77      1000\n",
      "weighted avg       0.85      0.85      0.85      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.92       784\n",
      "         1.0       0.72      0.69      0.71       216\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.82      0.81      0.81      1000\n",
      "weighted avg       0.87      0.88      0.87      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.91      0.92       792\n",
      "         1.0       0.67      0.71      0.69       208\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.80      0.81      0.80      1000\n",
      "weighted avg       0.87      0.87      0.87      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.90      0.90       787\n",
      "         1.0       0.63      0.65      0.64       213\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.77      0.78      0.77      1000\n",
      "weighted avg       0.85      0.85      0.85      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.90      0.88       757\n",
      "         1.0       0.64      0.57      0.60       243\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.75      0.73      0.74      1000\n",
      "weighted avg       0.81      0.82      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y1_pred = mlp1.predict(val_datas)\n",
    "print(classification_report(val_day1_labels, y1_pred)) \n",
    "y2_pred = mlp2.predict(val_datas)\n",
    "print(classification_report(val_day2_labels, y2_pred)) \n",
    "y3_pred = mlp3.predict(val_datas)\n",
    "print(classification_report(val_day3_labels, y3_pred)) \n",
    "y4_pred = mlp4.predict(val_datas)\n",
    "print(classification_report(val_day4_labels, y4_pred)) \n",
    "y5_pred = mlp5.predict(val_datas)\n",
    "print(classification_report(val_day5_labels, y5_pred)) \n",
    "y6_pred = mlp6.predict(val_datas)\n",
    "print(classification_report(val_day6_labels, y6_pred)) \n",
    "y7_pred = mlp7.predict(val_datas)\n",
    "print(classification_report(val_day7_labels, y7_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
